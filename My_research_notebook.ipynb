{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# Agents Lab Notebook v1.0.0\nThis notebook contains steps and code to demonstrate the use of agents\nconfigured in Agent Lab in watsonx.ai. It introduces Python API commands\nfor authentication using API key and invoking a LangGraph agent with a watsonx chat model.\n\n**Note:** Notebook code generated using Agent Lab will execute successfully.\nIf code is modified or reordered, there is no guarantee it will successfully execute.\nFor details, see: <a href=\"/docs/content/wsj/analyze-data/fm-prompt-save.html?context=wx\" target=\"_blank\">Saving your work in Agent Lab as a notebook.</a>\n\nSome familiarity with Python is helpful. This notebook uses Python 3.11.\n\n## Notebook goals\nThe learning goals of this notebook are:\n\n* Defining a Python function for obtaining credentials from the IBM Cloud personal API key\n* Creating an agent with a set of tools using a specified model and parameters\n* Invoking the agent to generate a response \n\n# Setup", "metadata": {}}, {"cell_type": "code", "source": "# import dependencies\nfrom langchain_ibm import ChatWatsonx\nfrom ibm_watsonx_ai import APIClient\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.prebuilt import create_react_agent\nfrom ibm_watsonx_ai.foundation_models.utils import Tool, Toolkit\nimport json\nimport requests", "metadata": {"id": "b4f62f07-65fd-4eae-b192-ad87211c3b7b"}, "outputs": [], "execution_count": 1}, {"cell_type": "markdown", "source": "## watsonx API connection\nThis cell defines the credentials required to work with watsonx API for Foundation\nModel inferencing.\n\n**Action:** Provide the IBM Cloud personal API key. For details, see\n<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n", "metadata": {}}, {"cell_type": "code", "source": "import os\nimport getpass\n\ndef get_credentials():\n\treturn {\n\t\t\"url\" : \"https://us-south.ml.cloud.ibm.com\",\n\t\t\"apikey\" : getpass.getpass(\"Please enter your api key (hit enter): \")\n\t}\n\ndef get_bearer_token():\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    data = f\"grant_type=urn:ibm:params:oauth:grant-type:apikey&apikey={credentials['apikey']}\"\n\n    response = requests.post(url, headers=headers, data=data)\n    return response.json().get(\"access_token\")\n\ncredentials = get_credentials()", "metadata": {"id": "b6722b79-2ee1-408a-8bd1-69c3f550b733"}, "outputs": [{"output_type": "stream", "name": "stdin", "text": "Please enter your api key (hit enter):  \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"}], "execution_count": 3}, {"cell_type": "markdown", "source": "# Using the agent\nThese cells demonstrate how to create and invoke the agent\nwith the selected models, tools, and parameters.\n\n## Defining the model id\nWe need to specify model id that will be used for inferencing:", "metadata": {}}, {"cell_type": "code", "source": "model_id = \"meta-llama/llama-3-3-70b-instruct\"", "metadata": {"id": "0156b9ab-881d-49bd-8dd8-9ad9128245ed"}, "outputs": [], "execution_count": 4}, {"cell_type": "markdown", "source": "## Defining the model parameters\nWe need to provide a set of model parameters that will influence the\nresult:", "metadata": {}}, {"cell_type": "code", "source": "parameters = {\n    \"frequency_penalty\": 0,\n    \"max_tokens\": 2000,\n    \"presence_penalty\": 0,\n    \"temperature\": 0,\n    \"top_p\": 1\n}", "metadata": {"id": "5e20168a-9550-41c2-a0fa-c3820ec54a99"}, "outputs": [], "execution_count": 5}, {"cell_type": "markdown", "source": "## Defining the project id or space id\nThe API requires project id or space id that provides the context for the call. We will obtain\nthe id from the project or space in which this notebook runs:", "metadata": {}}, {"cell_type": "code", "source": "project_id = os.getenv(\"PROJECT_ID\")\nspace_id = os.getenv(\"SPACE_ID\")\n", "metadata": {"id": "7e941ce3-81f2-44c2-ab58-3032dee32dce"}, "outputs": [], "execution_count": 6}, {"cell_type": "markdown", "source": "## Creating the agent\nWe need to create the agent using the properties we defined so far:", "metadata": {}}, {"cell_type": "code", "source": "client = APIClient(credentials=credentials, project_id=project_id, space_id=space_id)\n\n# Create the chat model\ndef create_chat_model():\n    chat_model = ChatWatsonx(\n        model_id=model_id,\n        url=credentials[\"url\"],\n        space_id=space_id,\n        project_id=project_id,\n        params=parameters,\n        watsonx_client=client,\n    )\n    return chat_model", "metadata": {"id": "5e6289eb-f2fa-4a45-a82a-25ab1e0213ba"}, "outputs": [], "execution_count": 7}, {"cell_type": "code", "source": "from ibm_watsonx_ai.deployments import RuntimeContext\n\ncontext = RuntimeContext(api_client=client)\n\n\n\n\ndef create_utility_agent_tool(tool_name, params, api_client, **kwargs):\n    from langchain_core.tools import StructuredTool\n    utility_agent_tool = Toolkit(\n        api_client=api_client\n    ).get_tool(tool_name)\n\n    tool_description = utility_agent_tool.get(\"description\")\n\n    if (kwargs.get(\"tool_description\")):\n        tool_description = kwargs.get(\"tool_description\")\n    elif (utility_agent_tool.get(\"agent_description\")):\n        tool_description = utility_agent_tool.get(\"agent_description\")\n    \n    tool_schema = utility_agent_tool.get(\"input_schema\")\n    if (tool_schema == None):\n        tool_schema = {\n            \"type\": \"object\",\n            \"additionalProperties\": False,\n            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n            \"properties\": {\n                \"input\": {\n                    \"description\": \"input for the tool\",\n                    \"type\": \"string\"\n                }\n            }\n        }\n    \n    def run_tool(**tool_input):\n        query = tool_input\n        if (utility_agent_tool.get(\"input_schema\") == None):\n            query = tool_input.get(\"input\")\n\n        results = utility_agent_tool.run(\n            input=query,\n            config=params\n        )\n        \n        return results.get(\"output\")\n    \n    return StructuredTool(\n        name=tool_name,\n        description = tool_description,\n        func=run_tool,\n        args_schema=tool_schema\n    )\n\n\ndef create_custom_tool(tool_name, tool_description, tool_code, tool_schema, tool_params):\n    from langchain_core.tools import StructuredTool\n    import ast\n\n    def call_tool(**kwargs):\n        tree = ast.parse(tool_code, mode=\"exec\")\n        custom_tool_functions = [ x for x in tree.body if isinstance(x, ast.FunctionDef) ]\n        function_name = custom_tool_functions[0].name\n        compiled_code = compile(tree, 'custom_tool', 'exec')\n        namespace = tool_params if tool_params else {}\n        exec(compiled_code, namespace)\n        return namespace[function_name](**kwargs)\n        \n    tool = StructuredTool(\n        name=tool_name,\n        description = tool_description,\n        func=call_tool,\n        args_schema=tool_schema\n    )\n    return tool\n\ndef create_custom_tools():\n    custom_tools = []\n\n\ndef create_tools(context):\n    tools = []\n    \n    config = None\n    tools.append(create_utility_agent_tool(\"GoogleSearch\", config, client))\n    config = {\n    }\n    tools.append(create_utility_agent_tool(\"DuckDuckGo\", config, client))\n\n    return tools", "metadata": {"id": "e9364979-a42f-4a48-b1b3-30a74a976033"}, "outputs": [], "execution_count": 8}, {"cell_type": "code", "source": "def create_agent(context):\n    # Initialize the agent\n    chat_model = create_chat_model()\n    tools = create_tools(context)\n\n    memory = MemorySaver()\n    instructions = \"\"\"# Notes\n- Use markdown syntax for formatting code snippets, links, JSON, tables, images, files.\n- Any HTML tags must be wrapped in block quotes, for example ```<html>```.\n- When returning code blocks, specify language.\n- Sometimes, things don't go as planned. Tools may not provide useful information on the first few tries. You should always try a few different approaches before declaring the problem unsolvable.\n- When the tool doesn't give you what you were asking for, you must either use another tool or a different tool input.\n- When using search engines, you try different formulations of the query, possibly even in a different language.\n- You cannot do complex calculations, computations, or data manipulations without using tools.\n- If you need to call a tool to compute something, always call it instead of saying you will call it.\n\nIf a tool returns an IMAGE in the result, you must include it in your answer as Markdown.\n\nExample:\n\nTool result: IMAGE({commonApiUrl}/wx/v1-beta/utility_agent_tools/cache/images/plt-04e3c91ae04b47f8934a4e6b7d1fdc2c.png)\nMarkdown to return to user: ![Generated image]({commonApiUrl}/wx/v1-beta/utility_agent_tools/cache/images/plt-04e3c91ae04b47f8934a4e6b7d1fdc2c.png)\n\n You are a research Agent, an AI system designed to assist with academic and scientific research tasks. You can autonomously search for literature, summarize papers, and organize references. \nUsing natural language processing, you understand research questions and retrieve relevant information. \nYou can generate reports, suggest hypotheses, and even draft sections of research papers. \nYou save time by automating repetitive tasks like citation management and data extraction. \nYour job is to enhance efficiency, accuracy, and innovation in both academic and industrial R&D.\"\"\"\n\n    agent = create_react_agent(chat_model, tools=tools, checkpointer=memory, state_modifier=instructions)\n\n    return agent", "metadata": {"id": "28d17f8a-0efa-431b-bd90-5b9f2ea9ca62"}, "outputs": [], "execution_count": 9}, {"cell_type": "code", "source": "# Visualize the graph\nfrom IPython.display import Image, display\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n\nImage(\n    create_agent(context).get_graph().draw_mermaid_png(\n        draw_method=MermaidDrawMethod.API,\n    )\n)\n", "metadata": {"id": "b91d561d-751c-49fc-aade-bb219cc5ad4f"}, "outputs": [{"execution_count": 10, "output_type": "execute_result", "data": {"image/png": "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", "text/plain": "<IPython.core.display.Image object>"}, "metadata": {}}], "execution_count": 10}, {"cell_type": "markdown", "source": "## Invoking the agent\nLet us now use the created agent, pair it with the input, and generate the response to your question:\n", "metadata": {}}, {"cell_type": "code", "source": "agent = create_agent(context)\n\ndef convert_messages(messages):\n    converted_messages = []\n    for message in messages:\n        if (message[\"role\"] == \"user\"):\n            converted_messages.append(HumanMessage(content=message[\"content\"]))\n        elif (message[\"role\"] == \"assistant\"):\n            converted_messages.append(AIMessage(content=message[\"content\"]))\n    return converted_messages\n\nquestion = input(\"Question: \")\n\nmessages = [{\n    \"role\": \"user\",\n    \"content\": question\n}]\n\ngenerated_response = agent.invoke(\n    { \"messages\": convert_messages(messages) },\n    { \"configurable\": { \"thread_id\": \"42\" } }\n)\n\nprint_full_response = False\n\nif (print_full_response):\n    print(generated_response)\nelse:\n    result = generated_response[\"messages\"][-1].content\n    print(f\"Agent: {result}\")\n", "metadata": {"id": "dd13b338-5605-4512-898f-c2b5a516e280"}, "outputs": [{"output_type": "stream", "name": "stdin", "text": "Question:  Prepare a report for RAG develpoment for research\n"}, {"name": "stderr", "text": "Failure during chat. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-07-09)\nStatus code: 403, body: {\"errors\":[{\"code\":\"invalid_instance_status_error\",\"message\":\"WML instance 9a466df5-a180-4ba7-94f1-d4be859db626 status is invalid, current status: Inactive, expected status: active\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai#text-chat\"}],\"trace\":\"65d6dd5b9c806a2cf7e0d676e86a2486\",\"status_code\":403}\n", "output_type": "stream"}, {"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mApiRequestFailure\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m     12\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: question\n\u001b[1;32m     17\u001b[0m }]\n\u001b[0;32m---> 19\u001b[0m generated_response \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m     20\u001b[0m     { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: convert_messages(messages) },\n\u001b[1;32m     21\u001b[0m     { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m42\u001b[39m\u001b[38;5;124m\"\u001b[39m } }\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m print_full_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (print_full_response):\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1600\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1599\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1602\u001b[0m     config,\n\u001b[1;32m   1603\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1604\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1605\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1606\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1607\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1609\u001b[0m ):\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1611\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1328\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1323\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1324\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1325\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1326\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1327\u001b[0m     ):\n\u001b[0;32m-> 1328\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1329\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1330\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1331\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1332\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1333\u001b[0m         ):\n\u001b[1;32m   1334\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langgraph/pregel/runner.py:58\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     56\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     run_with_retry(t, retry_policy)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langgraph/utils/runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langgraph/utils/runnable.py:176\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m    175\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m--> 176\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    178\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langgraph/prebuilt/chat_agent_executor.py:533\u001b[0m, in \u001b[0;36mcreate_react_agent.<locals>.call_model\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_model\u001b[39m(state: AgentState, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AgentState:\n\u001b[0;32m--> 533\u001b[0m     response \u001b[38;5;241m=\u001b[39m model_runnable\u001b[38;5;241m.\u001b[39minvoke(state, config)\n\u001b[1;32m    534\u001b[0m     has_tool_calls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(response, AIMessage) \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtool_calls\n\u001b[1;32m    535\u001b[0m     all_tools_return_direct \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;28mall\u001b[39m(call[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m should_return_direct \u001b[38;5;28;01mfor\u001b[39;00m call \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtool_calls)\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, AIMessage)\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     )\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langchain_core/runnables/base.py:3034\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3033\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3034\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langchain_core/runnables/base.py:5416\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5409\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5411\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5414\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5415\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   5417\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   5418\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   5419\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   5420\u001b[0m     )\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:368\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    364\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    365\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    369\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    370\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    371\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    372\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    373\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    374\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    375\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    376\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    377\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    378\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:937\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    936\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:759\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    758\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 759\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    760\u001b[0m                 m,\n\u001b[1;32m    761\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    762\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    763\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    764\u001b[0m             )\n\u001b[1;32m    765\u001b[0m         )\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1002\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1002\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m   1003\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1004\u001b[0m     )\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1006\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/langchain_ibm/chat_models.py:607\u001b[0m, in \u001b[0;36mChatWatsonx._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[1;32m    605\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 607\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwatsonx_model\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    608\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: params})\n\u001b[1;32m    609\u001b[0m )\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/model_inference.py:347\u001b[0m, in \u001b[0;36mModelInference.chat\u001b[0;34m(self, messages, params, tools, tool_choice, tool_choice_option, context)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_id:\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WMLClientError(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `context` parameter is only supported for inferring a chat prompt deployment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m     )\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    348\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    349\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    350\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    351\u001b[0m     tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[1;32m    352\u001b[0m     tool_choice_option\u001b[38;5;241m=\u001b[39mtool_choice_option,\n\u001b[1;32m    353\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    354\u001b[0m )\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/fm_model_inference.py:153\u001b[0m, in \u001b[0;36mFMModelInference.chat\u001b[0;34m(self, messages, params, tools, tool_choice, tool_choice_option, context)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    143\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m     context: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    149\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    151\u001b[0m     text_chat_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_href_definitions\u001b[38;5;241m.\u001b[39mget_fm_chat_href(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_chat_payload(\n\u001b[1;32m    154\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    155\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    156\u001b[0m         generate_url\u001b[38;5;241m=\u001b[39mtext_chat_url,\n\u001b[1;32m    157\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    158\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[1;32m    159\u001b[0m         tool_choice_option\u001b[38;5;241m=\u001b[39mtool_choice_option,\n\u001b[1;32m    160\u001b[0m     )\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/base_model_inference.py:370\u001b[0m, in \u001b[0;36mBaseModelInference._send_chat_payload\u001b[0;34m(self, messages, params, generate_url, tools, tool_choice, tool_choice_option)\u001b[0m\n\u001b[1;32m    361\u001b[0m post_params: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    362\u001b[0m     url\u001b[38;5;241m=\u001b[39mgenerate_url,\n\u001b[1;32m    363\u001b[0m     json\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    364\u001b[0m     params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_params(skip_for_create\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, skip_userfs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    365\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_get_headers(),\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    368\u001b[0m response_scoring \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_client, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpost_params)\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_response(\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    373\u001b[0m     response_scoring,\n\u001b[1;32m    374\u001b[0m     _field_to_hide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    375\u001b[0m )\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_watsonx_ai/wml_resource.py:168\u001b[0m, in \u001b[0;36mWMLResource._handle_response\u001b[0;34m(self, expected_status_code, operationName, response, json_response, _silent_response_logging, _field_to_hide)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiRequestFailure(\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailure during \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(operationName),\n\u001b[1;32m    170\u001b[0m         response,\n\u001b[1;32m    171\u001b[0m     )\n", "\u001b[0;31mApiRequestFailure\u001b[0m: Failure during chat. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-07-09)\nStatus code: 403, body: {\"errors\":[{\"code\":\"invalid_instance_status_error\",\"message\":\"WML instance 9a466df5-a180-4ba7-94f1-d4be859db626 status is invalid, current status: Inactive, expected status: active\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai#text-chat\"}],\"trace\":\"65d6dd5b9c806a2cf7e0d676e86a2486\",\"status_code\":403}"], "ename": "ApiRequestFailure", "evalue": "Failure during chat. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/chat?version=2025-07-09)\nStatus code: 403, body: {\"errors\":[{\"code\":\"invalid_instance_status_error\",\"message\":\"WML instance 9a466df5-a180-4ba7-94f1-d4be859db626 status is invalid, current status: Inactive, expected status: active\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai#text-chat\"}],\"trace\":\"65d6dd5b9c806a2cf7e0d676e86a2486\",\"status_code\":403}", "output_type": "error"}], "execution_count": 11}, {"cell_type": "markdown", "source": "# Next steps\nYou successfully completed this notebook! You learned how to use\nwatsonx.ai inferencing SDK to generate response from the foundation model\nbased on the provided input, model id and model parameters. Check out the\nofficial watsonx.ai site for more samples, tutorials, documentation, how-tos, and blog posts.\n\n<a id=\"copyrights\"></a>\n### Copyrights\n\nLicensed Materials - Copyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the ILAN License.\nUse, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\n**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for watsonx.ai Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n\nBy downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  ", "metadata": {}}, {"cell_type": "code", "source": "", "metadata": {"id": "789cf80c-4d43-473d-b46c-8f946635f5f0"}, "outputs": [], "execution_count": null}]}